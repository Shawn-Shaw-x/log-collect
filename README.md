> åœ¨åˆ†å¸ƒå¼åœºæ™¯ä¸‹ï¼Œæ—¥å¿—çš„æ”¶é›†å’Œæ£€ç´¢å˜å¾—é¢å¤–å›°éš¾ã€‚æœ¬æ–‡å°†åˆ©ç”¨goçš„åç¨‹å®ç°é«˜ååé‡çš„åˆ†å¸ƒå¼æ—¥å¿—æ”¶é›†ï¼Œç»“åˆetcdåšåˆ°é…ç½®çƒ­æ›´æ–°ã€åˆ©ç”¨kafkaå¼‚æ­¥å‘é€æ—¥å¿—å†…å®¹åˆ°elasticSearchä¸­ã€‚å®ç°æ—¥å¿—å†…å®¹çš„ç±»å®æ—¶åˆ·æ–°ï¼Œæ–¹ä¾¿åˆ†å¸ƒå¼ç³»ç»Ÿçš„æŸ¥çœ‹å’Œæ£€ç´¢æ—¥å¿—å†…å®¹ã€‚

## åˆ†å¸ƒå¼æ—¥å¿—ç³»ç»Ÿç®€ä»‹
### â­ï¸1.ç³»ç»Ÿæ¡†æ¶
æœ¬ç³»ç»Ÿæ­å»ºæ¶‰åŠä¸¤ä¸ªç‹¬ç«‹ç¨‹åºã€‚**log-collect**å’Œ**logtransfer**ï¼Œä»¥**kafka**ä½œä¸ºä¸¤è€…ä¹‹é—´çš„æ¶ˆæ¯é˜Ÿåˆ—ã€‚å…¶ä¸­log-collectä½œä¸ºè¾“å…¥ç«¯ç”¨äºå¤šæœºæ—¥å¿—æ”¶é›†ã€‚logtransferä½œä¸ºè¾“å‡ºç«¯ï¼Œå°†kafkaä¸­çš„å†…å®¹è¾“å‡ºåˆ°elasticSearchã€‚æ¶‰åŠç¬¬ä¸‰æ–¹ç»„ä»¶å¦‚åˆ—è¡¨æ‰€ç¤ºï¼š

- etcdï¼šç”¨äºé…ç½®æ›´æ–°
- kafkaï¼šlog-collectå’Œlogtranserä¹‹é—´è¿›è¡Œå¼‚æ­¥é€šä¿¡
- elasticSearchï¼šç”¨äºæ—¥å¿—å­˜å‚¨
- kibanaï¼šæä¾›æ•°æ®å±•ç¤ºå’Œæœç´¢çš„å‰å°é¡µé¢

ç³»ç»Ÿæ•´ä½“æ¡†æ¶å›¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://img.learnblockchain.cn/attachments/2025/04/TC7KqUVc67ed1aa5033f9.png)

### â­ï¸2.ä¾èµ–æ­å»º
æœ¬ç³»ç»Ÿæ­å»ºæ‰€éœ€ä¾èµ–ï¼šdockerã€etcdã€kafkaã€elasticsearchã€kibanaã€‚
é¦–å…ˆï¼Œå¦‚æœä½ çš„æœºå™¨ä¸Šæ²¡æœ‰dockerç¯å¢ƒï¼Œéœ€è¦å…ˆå®‰è£…dockerã€‚
[dockerå®˜ç½‘](https://www.docker.com/products/docker-desktop/ )



åœ¨é…ç½®å®Œdockerç¯å¢ƒåï¼Œå¯ä»¥é‡‡ç”¨docker compose upå‘½ä»¤æ‰§è¡Œymlæ–‡ä»¶ï¼Œè¿›è¡Œä¸€é”®æ‹‰å–å¹¶è¿è¡Œç›¸å…³å®¹å™¨ã€‚
```
services:
  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    restart: always
    ports:
      - "9092:9092"
      - "9093:9093"  # KRaft é€‰ä¸¾ç«¯å£
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@localhost:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
  etcd:
    image: bitnami/etcd:3
    container_name: etcd
    restart: unless-stopped
    ports:
      - "2379:2379"
      - "2380:2380"
    environment:
      - ALLOW_NONE_AUTHENTICATION=yes
      - ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380
      - ETCD_INITIAL_ADVERTISE_PEER_URLS=http://0.0.0.0:2380
      - ETCD_INITIAL_CLUSTER=default=http://0.0.0.0:2380
      - ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster
      - ETCD_INITIAL_CLUSTER_STATE=new
    volumes:
      - etcd-data:/etcd-data
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.6.2
    container_name: elasticsearch
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - esnet
    ports:
      - 9200:9200
      - 9300:9300
    restart: always

  kibana:
    image: docker.elastic.co/kibana/kibana:8.6.2
    container_name: kibana
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    ports:
      - 5601:5601
    networks:
      - esnet
    restart: always
volumes:
  etcd-data:
  es_data:
    driver: local
  kafka_data:
    driver: local
networks:
  esnet:
    driver: bridge

 ```
å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œå³å¯åœ¨docker destopä¸­çœ‹åˆ°

![image.png](https://img.learnblockchain.cn/attachments/2025/04/KqW8ZovJ67ed205617a0f.png)
ä¸‹é¢ï¼Œæˆ‘ä»¬å°†é€ä¸€æµ‹è¯•å„æœåŠ¡æ˜¯å¦æ­£å¸¸

- etcdï¼š
  è¾“å…¥ï¼š`etcdctl --endpoints=http://127.0.0.1:2379 endpoint health`
  å¦‚æœæ­£å¸¸ï¼Œä½ å°†çœ‹åˆ°ï¼š`127.0.0.1:2379 is healthy: successfully committed proposal`
- kafkaï¼š
  è¿›å…¥kafkaå®¹å™¨ï¼š `docker exec -it kafka /bin/bash`
  è¿›å…¥binç›®å½•ï¼š`/opt/bitnami/kafka/bin`
  æ‰§è¡Œproducerï¼š` kafka-console-producer.sh --bootstraprver 127.0.0.1:9092 --topic test_topic`
  å¦‚æœæ­£å¸¸ï¼Œä½ å°†è¿›å…¥åˆ°æ§åˆ¶å°å†…ï¼Œå¯è¾“å…¥ç”Ÿäº§è€…æ¶ˆæ¯ã€‚
- elasticsearchï¼š
  è¾“å…¥ï¼š`curl -X GET "http://localhost:9200/"`
  å¦‚æœæ­£å¸¸ï¼Œä½ å°†çœ‹åˆ°ï¼š
```
{
  "name" : "f92a87d6387e",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "e-cjW93ERSqlJJ0qFLWclA",
  "version" : {
    "number" : "8.6.2",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "2d58d0f136141f03239816a4e360a8d17b6d8f29",
    "build_date" : "2023-02-13T09:35:20.314882762Z",
    "build_snapshot" : false,
    "lucene_version" : "9.4.2",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
```
- kibanaï¼š
  æµè§ˆå™¨ä¸Šè®¿é—®[kibanaé¡µé¢](http://localhost:5601/)
  å¦‚æœæ­£å¸¸ï¼Œä½ å°†è¿›å…¥åˆ°kibanaçš„é¡µé¢

### â­ï¸3. log-agentæ­å»º
åœ¨æ­å»ºlog-agentä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆæ¥ç”»ä¸ªå›¾æ¥åˆ†æä¸‹log-agentæœåŠ¡æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Œç„¶åæˆ‘ä»¬é’ˆå¯¹å›¾ä¸­çš„æ ¸å¿ƒè¦ç‚¹é€ä¸€åˆ†æã€‚

![image.png](https://img.learnblockchain.cn/attachments/2025/04/SURE08ZG67ed2de1a4837.png)

#### 3.1 å¯åŠ¨kafka producerè¿æ¥å‘é€æ¶ˆæ¯åˆ°kafka
åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œä¸»è¦ä»»åŠ¡æ˜¯åˆ©ç”¨saramaå·¥å…·åŒ…ï¼Œå»å¯åŠ¨ä¸€ä¸ªkafka producerå»å»ºç«‹å’Œkafkaçš„è¿æ¥ï¼Œå¯åŠ¨ä¸€ä¸ªgoçš„åç¨‹ï¼ˆå½“ç„¶å¯ä»¥å¯å¤šä¸ªï¼Œè¿™æ­¥å¯æ‹“å±•ï¼‰å»ä¸æ–­ç›‘å¬msgChançš„æ¶ˆæ¯ç®¡é“ï¼Œå¦‚æœæœ‰æ¶ˆæ¯ï¼Œåˆ™è¯»å–å¹¶å‘é€åˆ°kafkaæ¶ˆæ¯é˜Ÿåˆ—ä¸­ã€‚

```go
// init kafka client
func Init(addresses []string, chanSize int64) error {
	fmt.Println("Kafka Client")
	saramaConfig := sarama.NewConfig()
	saramaConfig.Producer.RequiredAcks = sarama.WaitForAll          // wait for all partition
	saramaConfig.Producer.Partitioner = sarama.NewRandomPartitioner // random partition
	saramaConfig.Producer.Return.Successes = true

	// new producer for send message to kafka
	client, err := sarama.NewSyncProducer(addresses, saramaConfig)
	if err != nil {
		logrus.Error("init kafka failed.", err.Error())
		return err
	}

	//init msg chan
	msgChan = make(chan *sarama.ProducerMessage, chanSize)

	Client = client

	// read message from chan (tail)
	go SendMsg2Kfk()

	return nil
}

// send message to channel(kafka)
func SendMsg2Kfk() error {
	for {
		select {
		case msg := <-msgChan:
			partition, offset, err := Client.SendMessage(msg)
			if err != nil {
				logrus.Error("send message failed.", err.Error())
				return err
			}
			logrus.Infof("send message to partition %d, offset %d", partition, offset)
		}
	}
	return nil
}
```
#### 3.2 å¯åŠ¨etcd çƒ­æ›´æ–°é…ç½®

åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦åšçš„ä»»åŠ¡æœ‰ï¼š
- å¯åŠ¨ä¸€ä¸ªetcdçš„è¿æ¥ï¼Œwatch ä¸€ä¸ªetcd ä¸­çš„ä¸€ä¸ªkeyï¼Œè¿”å›ä¸€ä¸ªé…ç½®ç®¡é“watchChan
- å¦‚æœwatchChanä¸­æœ‰é…ç½®æ¶ˆæ¯çš„æ›´æ–°ï¼Œåˆ™å‘é€åˆ°é…ç½®ç®¡é“ä¸­ï¼Œä¾›è¯»å–æ–‡ä»¶ä»»åŠ¡ä½¿ç”¨
```go

// init etcd client
func Init(address []string) error {
	//ctx, cancel := context.WithTimeout(context.Background(), time.Second*5)
	cli, err := clientv3.New(clientv3.Config{
		Endpoints:   address,
		DialTimeout: 5 * time.Second,
	})
	if err != nil {
		fmt.Println("cannot connect to etcd", err)
		return err
	}
	//defer cli.Close()
	client = cli
	return nil
}

func GetConf(conf string, newConfigChan chan *[]common.CollectEntry) (err error) {
	//ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	//defer cancel()
	firstTime := atomic.Bool{}
	firstTime.Store(true)
	go func() {
		ctx := context.Background()
		watchChan := client.Watch(ctx, "/"+conf)
		defer client.Close()
		for {
			// if first time in this loop, get config from etcd
			if firstTime.Load() {
				newConfigChan = getConfigFromEtcd(ctx, conf, newConfigChan)
			}

			select {
			case resp := <-watchChan:
				firstTime.Store(false)
				for _, ev := range resp.Events {
					logrus.WithFields(logrus.Fields{
						"key":     string(ev.Kv.Key),
						"value":   string(ev.Kv.Value),
						"version": ev.Kv.Version,
						"type":    ev.Type,
					}).Info("watch event")
					configs := new([]common.CollectEntry)
					json.Unmarshal(ev.Kv.Value, configs)
					newConfigChan <- configs
				}

			case <-ctx.Done():
			}
		}
	}()

	return nil

}

func getConfigFromEtcd(ctx context.Context, conf string, newConfigChan chan *[]common.CollectEntry) chan *[]common.CollectEntry {
	response, err2 := client.Get(ctx, "/"+conf)
	if err2 != nil {
		logrus.Error("first time get confs err", err2)
	}
	for index, value := range response.Kvs {
		logrus.WithFields(logrus.Fields{
			"index": index,
			"key":   string(value.Key),
			"value": string(value.Value),
		}).Info("first time get from etcd,")
		configs := new([]common.CollectEntry)
		json.Unmarshal(value.Value, configs)
		newConfigChan <- configs
	}
	return newConfigChan
}

```

#### 3.3 å¯åŠ¨è¯»å–æ–‡ä»¶å¹¶å‘é€åˆ°æ¶ˆæ¯ç®¡é“
åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åšçš„æœ‰ï¼š
- å¯åŠ¨ä¸€ä¸ªåç¨‹ï¼Œä¸æ–­ç›‘å¬é…ç½®ç®¡é“ä¸­çš„é…ç½®æ›´æ–°
- å¦‚æœæœ‰é…ç½®çš„æ›´æ–°ï¼Œå…ˆåœæ­¢ä¸Šä¸€æ‰¹æ¬¡è´Ÿè´£è¯»å–æ–‡ä»¶çš„åç¨‹ï¼Œç„¶åæ ¹æ®é…ç½®é‡Œé¢çš„æ¡æ•°ï¼Œæ–°å»ºè¿™ä¸€æ‰¹è¯»å–æ–‡ä»¶çš„åç¨‹ï¼Œä¸æ–­è¯»å–æ—¥å¿—ï¼Œç„¶åå‘é€åˆ°æ¶ˆæ¯ç®¡é“ä¸­ï¼Œä¾›kafka producerä»»åŠ¡å»å‘é€åˆ°kafkaä¸­ã€‚


ğŸ“… è¿™ä¸€æ­¥çš„éš¾ç‚¹ä¸»è¦åœ¨äºæˆ‘ä»¬å…ˆè¦ä½¿ç”¨ä¸€ä¸ªåç¨‹ç›‘å¬é…ç½®çš„æ›´æ–°ï¼Œç„¶ååœ¨è¿™ä¸ªåç¨‹ä¸­åˆæŒ‰ç…§é…ç½®çš„å˜æ›´å»å¯åŠ¨ä¸€æ‰¹å­åç¨‹ã€‚åœ¨è¿™é‡Œï¼Œå…ˆæ˜¯ä½¿ç”¨äº†contextçš„æœºåˆ¶å»ç¼“å­˜ä¸Šä¸€æ‰¹åç¨‹å¯åŠ¨æ—¶çš„cancel(), åœ¨é…ç½®çš„æ›´æ–°æ—¶ï¼Œè°ƒç”¨cancel()ï¼Œé€šçŸ¥ä¸Šä¸€æ‰¹åç¨‹å»æ³¨é”€ã€‚

```go

func (t *tailTask) readLines(ctx context.Context) {
	defer t.instance.Stop()
	// read new contents from file
	for {
		select {
		case msg, ok := <-t.instance.Lines:
			if !ok {
				logrus.Error("EOF")
				continue
			}
			// exclude the space and /n
			msg.Text = strings.TrimSpace(msg.Text)

			logrus.Info("sending message to kafka, message = ", msg.Text)

			// async send message to kafka using chan
			producerMessage := &sarama.ProducerMessage{}
			producerMessage.Topic = t.topic
			producerMessage.Value = sarama.StringEncoder(msg.Text)
			kafka.SendToMsgChan(producerMessage)
		case <-ctx.Done():
			logrus.Info("kill goroutine,ctx Done.")
			return
		}

	}
}

func NewTailTask(path, topic string) *tailTask {
	// config Tail
	config := tail.Config{
		ReOpen:    true,
		Follow:    true,
		Location:  &tail.SeekInfo{Offset: 0, Whence: 2},
		MustExist: false,
		Poll:      true,
	}

	tailFile, err := tail.TailFile(path, config)

	if err != nil {
		log.Fatalf("Error opening file: %v", err)
	}
	tt := &tailTask{
		path:     path,
		topic:    topic,
		instance: tailFile,
	}
	return tt
}

func Init(newConfigChan chan *[]common.CollectEntry) error {
	go func() {
		// cache the cancel() func
		var cancelLastTime context.CancelFunc
		for {
			select {
			// dead loop
			case confs := <-newConfigChan:
				// stop all  goroutines before new batch=
				if cancelLastTime != nil {
					cancelLastTime()
				}
				ctx, cancel := context.WithCancel(context.Background())
				// cache
				cancelLastTime = cancel
				// execute goroutine to read from file,
				// and sent to msgChan
				for _, conf := range *confs {
					tt := NewTailTask(conf.Path, conf.Topic)
					go tt.readLines(ctx)
				}
			}
		}

	}()
	return nil
}
```

### â­ï¸4. log-collectæ­å»º
åœ¨æ­å»ºlog-collectä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆæ¥ç”»ä¸ªå›¾æ¥åˆ†æä¸‹log-collectæœåŠ¡æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Œç„¶åæˆ‘ä»¬é’ˆå¯¹å›¾ä¸­çš„æ ¸å¿ƒè¦ç‚¹é€ä¸€åˆ†æã€‚
#### 4.1 å¯åŠ¨kafka consumerè·å–æ¶ˆæ¯
åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ–°å»ºä¸€ä¸ªkafka çš„consumerï¼Œ é€šè¿‡è¿™ä¸ªconsumerå»è·å–topicçš„partitionï¼Œç„¶åæ ¹æ®partitionçš„ä¸ªæ•°ï¼Œå»åˆ›å»ºå¤šä¸ªåç¨‹å¹¶å‘è·å–kafkaä¸­çš„æ¶ˆæ¯ã€‚

```go

// transfer messages from kafka to msgChan
func (k *KFKClient) AsyncReadMessageToChan(topic string, msgChan chan<- *sarama.ConsumerMessage) error {
	//  get patition list of this topic
	partitionList, err := k.Consumer.Partitions(topic)
	if err != nil {
		logrus.Errorf("Error getting list of partitions: %v", err)
		panic(err)
	}

	logrus.WithFields(logrus.Fields{
		"count": len(partitionList),
		"value": partitionList,
	}).Info("get list of partitions")

	ctx, cancel := context.WithCancel(context.Background())

	for _, partition := range partitionList {
		partitionConsumer, err := k.Consumer.ConsumePartition("web_log", partition, sarama.OffsetNewest)
		if err != nil {
			logrus.WithFields(logrus.Fields{
				"partition": partition,
				"error":     err,
			}).Error("Failed to consume partition")
			continue
		}
		// for a single partition,
		// execute goroutine to transfer message to msgChan
		go func(pc sarama.PartitionConsumer, p int32) {
			defer pc.AsyncClose()
			for {
				select {
				case msg := <-pc.Messages():
					logrus.WithFields(logrus.Fields{
						"partition": p,
						"message":   string(msg.Value),
					}).Info("Consume messages")
					msgChan <- msg
				case err := <-pc.Errors():
					logrus.Error(err)
					cancel()
					return
				case <-ctx.Done():
					logrus.Errorf("Got ctx.Done signal, shutting down")
					return
				}
			}
		}(partitionConsumer, partition)
	}
	return nil
}

```
#### 4.2 å¯åŠ¨elasticsearch clientå‘é€æ—¥å¿—
åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬æ‰€è¦åšçš„äº‹æ¯”è¾ƒç®€å•ï¼Œæˆ‘ä»¬æ ¹æ®é…ç½®é‡Œçš„ä¿¡æ¯ï¼Œåˆ›å»ºäº†100ä¸ªåç¨‹ï¼Œå¹¶å‘åœ°å°†ç®¡é“ä¸­çš„æ¶ˆæ¯å‘é€åˆ°elasticsearchä¸­ã€‚

```go

// transfer messages form msgChan to elasticsearch
func (c *ESClient) SendMsg2ESBatch(esConfig config.ESConfig, msgChan chan *sarama.ConsumerMessage) {
	ctx, cancel := context.WithCancel(context.Background())
	// execute 100 goroutines to send request to elasticsearch
	for i := 0; i < esConfig.GoroutineSize; i++ {
		go func(goroutineSize int) {
			for {
				select {
				case msg := <-msgChan:
					// send to es
					ret, err := c.ESProducer.Index(
						esConfig.Index,
						bytes.NewReader(msg.Value),
					)
					if err != nil {
						logrus.Errorf("send msg to es fail")
					}
					retBytes, _ := json.Marshal(ret)
					logrus.WithFields(logrus.Fields{
						"res": string(retBytes),
					}).Info("index")
					defer ret.Body.Close()
				case <-ctx.Done():
					cancel()
					logrus.Error("context cancel, ES client exit")
					return
				}
			}
		}(i)

	}

}
```

![image.png](https://img.learnblockchain.cn/attachments/2025/04/AS3S3UK167ed3034c0c0e.png)

### â­ï¸5. æ•´åˆæµ‹è¯•
1. å†™å…¥é…ç½®åˆ°etcd
   åœ¨dockerå®¹å™¨é‡Œæ‰§è¡Œå‘½ä»¤ï¼š`etcdctl put /collect_log_conf '[{"path":"./xx.log","topic":"web_log"},{"path":"./xy.log","topic":"web_log"}]'`
3. åˆ›å»ºxx.logå’Œxy.log
   åœ¨log-agentæ ¹ç›®å½•ä¸‹æ–°å»ºxx.logä¸xy.logï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º

![image.png](https://img.learnblockchain.cn/attachments/2025/04/HJnIfkpx67ed255ecd046.png)
5. å¯åŠ¨log-agent

![image.png](https://img.learnblockchain.cn/attachments/2025/04/BcwBzkhD67ed25b2b1ccd.png)
7. å¯åŠ¨log-transfer

![image.png](https://img.learnblockchain.cn/attachments/2025/04/Mx1k5odW67ed25ed3b682.png)
9. åœ¨xx.logå’Œxy.logä¸­æ–°å¢ä¸€è¡Œæ—¥å¿—ï¼Œå¹¶ä¿å­˜
   `{"time":"123","code":"200","msg":"ok"}`
   `{"time":"321","code":"500","msg":"not ok"}`
11. è§‚å¯Ÿlog-agentæ—¥å¿—

![image.png](https://img.learnblockchain.cn/attachments/2025/04/DAdvYCaZ67ed281a57dfa.png)
13. è§‚å¯Ÿlog-transferæ—¥å¿—

![image.png](https://img.learnblockchain.cn/attachments/2025/04/kTeaWuxR67ed2831ba06a.png)
15. è§‚å¯Ÿkibanaé¡µé¢


![image.png](https://img.learnblockchain.cn/attachments/2025/04/fzIY3svz67ed28b4d7139.png)
å¦‚æœåœ¨kibanaé¡µé¢ä¸­çœ‹åˆ°æ­£ç¡®æ•°æ®ï¼Œæ•´åˆæµ‹è¯•æˆåŠŸã€‚